<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
  <script type="text/javascript" id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
  <script src="https://cdn.jsdelivr.net/npm/p5@1.9.0/lib/p5.js"></script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css">
  <meta property="og:type" content="website">
  <meta property="og:title" content="Grand oral : La racine carrée inverse rapide">
  <meta property="og:url" content="https://doggigo.github.io/site-nsi-grand-oral/">
  <meta property="og:image" content="https://tenor.com/view/borzoi-snoopa-lord-foog-dog-funny-gif-20125052">
  <title>L'algorirthme racine carrée inverse rapide</title>
</head>
<body>
  <h1>Mon grand Oral</h1>
  <h2>Problématique :</h2>
  <h2>"L'algorithme racine carrée inverse rapide : doit-on privilégier les performances en dépit d'un mauvais code ?"</h2>
  <ul id="sommaire">
    <h3>Sommaire</h3>
    <h4>Partie 1</h4>
    <li> <a href="#presentation">Présentation de l'algorithme</a></li>
    <li> <a href="#utilite">À quoi sert cet algorithme ?</a></li>
    <li><a href="#pourquoi">Pourquoi utilisait-on cet algorithme ?</a></li>
    <li><a href="#fonctionnement">Comment fonctionne l'algorithme ?</a></li>
    <li><a href="#interet">Cet algorithme est-il toujours intéressant ?</a></li>
    <h4>Partie 2</h4>
    <li><a href="#def-bon-mauvais">Qu'appelle-t-on du "bon"/"mauvais" code ?</a></li>
    <li><a href="#perf-ou-bon">Préférer les performances au mauvais code est-il toujours en vigueur aujourd'hui ? Si oui, dans quel cas ?</a></li>
    <h4>Conclusion</h4>
    <li><a href="#">Phrase de fin</a></li>
    <li><a href="#bibliographie">Bibliographie</a></li>
  </ul>
  <h3 id="presentation">Présentation</h3>
  <p class="txt">L'algorithme "fast inverse square root" est un algorithme créé par Greg Walsh, un développeur de la compagnie Ardent. 
    <br>
    Il se Repose sur les travaux de William Kahan ayant publié 10 ans avant montrant comment calculer une racine carrée en manipulant des bits et en utilisant la méthode de Newton.
    <br>
    L'algorithme a été repris par ses collègues dont un appelé Brian Hook qui l'a alors conservé et réutilisé dans son poste de développeur de jeux pour la compagnie Id Software, créateurs des jeux Doom et Quake.
    <br>
    Cet algorithme est alors devenu une icône dans le monde de l'optimisation lors de la publication Open Source du code du jeu Quake III, dans lequel il était utilisé.
</p>
<pre>
  <code class="language-clike">
    float q_rsqrt(float number){
      long i;
      float x2, y;
      const float threehalfs = 1.5F;
      
      x2 = number * 0.5F;
      y  = number;
      i  = * ( long * ) &y;                       // evil floating point bit level hacking
      i  = 0x5f3759df - ( i >> 1 );               // what the f*ck?
      y  = * ( float * ) &i;
      y  = y * ( threehalfs - ( x2 * y * y ) );   // 1st iteration
      // y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed
      
      return y;
    }
  </code>
</pre>
<p>Tester le code</p>
<p style="display:inline">Entrez une valeur :</p>
<input type="text" placeholder="10.5" step=0.000001 id="q-sqrt-input">
<button id="exec-q-sqrt">Exécuter</button>
<div class="code-result-div" id="q-sqrt-result-div">
  <p class="code-result" id="q-sqrt-result">Résultat</p>
</div>
  <h3>À quoi sert la racine carrée inverse ?</h3>
  <p class="txt">
  La racine carrée inverse est une fonction composée utilisée en géométrie, aussi bien dans l'espace que dans le plan.
    <br>
  Partons d'un vecteur \(\vec{v} = \pmatrix{x \\ y}\).
    <br>
  Nous voulons que la norme de ce vecteur soit égale à 1.
    <br>
  Ce processus s'appelle la normalisation.
    <br>
  Pour obtenir \(\hat{w}\) ; représentant le résultat de la normalisation de \(\vec{v}\), nous devons diviser chaque valeur de \(\vec{v}\) par 
    <br>
  Soit \(\alpha\) la norme euclidienne de \(\vec{v}\) :
  $$\alpha = \sqrt{x_{\vec{v}}^2 + y_{\vec{v}}^2}$$
    <br>
    Ainsi, on peut calculer \(\hat{w}\) :
    $$\hat{w} = \pmatrix{\frac{x_\vec{v}}{\alpha}\\\frac{y_\vec{v}}{\alpha}}$$
    <br>
    Maintenant, notre vecteur est normalisé. On appelle alors ce vecteur un vecteur "unitaire".
    \(\hat{w}$ est donc un vecteur unitaire colinéaire à $\vec v\)
    <br>
    Voici une démonstration de La normalisation d'un vecteur (ici en 2 dimensions).
    Vous verrez aussi par la même occasion que l'algorithme présenté est fiable et n'a une marche d'erreur.

  
  </p>

  <div id="demo-grid">
    <div id="demo">
    </div>
    <div id="animation-steps">
    </div>
  </div>
  <button id="normalize-button">Normaliser le vecteur</button>

  <h3 id="pourquoi">Pourquoi utilisait-on cet algorithme ?</h3>
  <i>En effet, "utilisait", car depuis les nouveaux processeurs et architectures, il n'y a plus besoin de notre algorithme 
  (à moins de faire des recherches sur les anciens ordinateurs ou de travailler sur des consoles rétro). L'algorithme ci-dessous fonctionnera même mieux avec les architecture modernes que notre algorithme q_rsqrt, il reste néanmoins une belle preuve d'ingénierie et ouvre un sujet intéressant sur l'optimisation</i>
  <p class="txt">Il est vrai que le code est très dur à comprendre.
    <br>
    Alors pourquoi n'utilisait-on pas un algorithme plus simple, tel que celui-ci ?
    <pre>
      <code class="language-clike">
        #include &#60;math.h&#62;
        float inverse_sqrt(float number) {
          return 1 / sqrt(number);
        }
      </code>
    </pre>
    La réponse est qu'il était <strong>LENT</strong> car les opérations racine carrée et division étaient toutes les deux compliquées à calculer pour un ordinateur de l'époque.
    <br>
    Contrairement à la multiplication et aux opérations "bitwise" (au niveau de la représentation binaire) qui, elles, sont très rapides.
    <br>
    En moyenne, q_rsqrt était à peu près 4 fois plus rapide que inverse_sqrt en présentant une précision de plus de 99% avec une seule itération de la méthode de Newton.
    <br>
  </p>
  <h3 id="fonctionnement">Comment fonctionne l'algorithme ?</h3>
  <h4>Le standard IEEE 754</h4>
  <p class="txt">
    Le standard IEEE 754 est le tout premier standard pour représenter des nombres flottants.
    <br>
    Voici son agencement :
    <br>
  </p>
  <img src="images/IEEE_754_Single_Floating_Point_Format.png" alt="1er bit : le signe ; 2-9emes bits : l'exposant ; 9-32emes bits : la mantisse">
  <i>Source : IEEE 754-1985 - Wikipedia</i>

  <p class="txt">C'est de cette manière que les nombres flottants sont représentés en C, s'inspirant de la notation scientifique:
    <br>
    \(a = M\times B^n\) avec "a" le nombre, "B" la base, "M" la mantisse (comprise entre 1 et B exclu) et "n" l'exposant
    <br>
    On peut donc distinguer 3 parties :
    <br>
    Le signe représenté par le dernier bit
    <br>
    L'exposant représenté par les 11 bits suivants
    <br>
    La mantisse, représenté par les 23 premiers bits.
    <br>
    <br>
    Ignorons le bit du signe puisque la norme au carré d'un vecteur ne peut jamais être nulle.
    Les bits représentant L'exposant (en base 2) doivent eux aussi être signés, dans ce cas, on utilisera une portée de -128 à 128 au lieu de 0 à 256
    Quant aux bits représentant la mantisse, nous voulons une valeur entre 1 et 2, soit entre 1 et 2 exclu
    Les créateurs de ce standard ont alors remarqué que peu importe le nombre que l'on veut, la mantisse aura toujours pour partie entière le nombre 1,
    Nous n'avons donc pas besoin de le stocker le 1 puisqu'il est toujours présent. Cela nous laisse alors 23 bits soit 2^23 nombres de précision.
    Ainsi, nous représentons un nombre flottant de la même manière que l'écriture scientifique.
    <br>
  </p>
  <h4>Manipuler un nombre binaire</h4>
  <p class="txt">
    Puisque maintenant nous possédons la mantisse M et l'exposant E, on peut retrouver l'écriture totale de notre chiffre :
    <br>
    Notre nombre E doit être au début de notre représentation binaire, on le multiplie donc par 2^23 afin qu'il soit après la mantisse.
    <br>
    Ce qui nous donne : \(a = 2^{23} \times E + M\)
    <br>
    Alors, pour récupérer notre nombre (en base binaire) à partir de la norme IEEE754, on a alors à effectuer cette formule :
    <br>
    $$(1 + \frac{M}{2^{23}})\times 2^{E-127}$$
    <br>
    On va essayer de le simplifier en prenant le logarithme de notre formule :
    $$\log_2{\Big(1 + \frac{M}{2^{23}}\times 2^{E-127}\Big)} = \log_2{\Big(1 + \frac{M}{2^{23}}\Big)} + E-127$$
    On va être bloqués à cette étape. Mais puisque le but de notre algorithme est d'avoir une approximation, on va pouvoir simplifier.
    <br>
    Car lorsque x est un petit nombre, on remarque que : \(\log_2(1 + x) \approx x\)
    <br>
    On peut admettre \(\mu\) un nombre choisi de manière arbitraire permettant de se rapprocher du résultat.
    <br>
    Ici, le créateur de l'algorithme a décidé que \(\mu = 0.0450466\) car il a conclu que ce nombre donnait, en moyenne, les résultats les plus proches pour les nombres entre 0 et 1. Donc :
    $$\log_2(1 + x) \approx x + \mu$$
    <i>Une meilleure valeur pour \(\mu\) serait plutôt aux alentours de 0.0430</i>
    <br>
    On va donc écrire la formule au complet avec notre approximation :
    $$\frac{M}{2^{23}} + \mu + E - 127 = \frac{1}{2^{23}}\times(\textcolor{red}{2^{23}\times E + M}) + \mu + 127$$
    On retrouve notre formule pour obtenir notre représentation binaire \(\textcolor{red}{2^{23}\times E + M}\)
    <br>
    Ainsi, en appliquant le logarithme binaire sur notre représentation binaire, on obtient notre formule de base, simplement décalée et additionnée à \(127 + \mu\)
    On peut donc dire que si n un nombre flottant, \(n \approx log_2(n)\)
  </p>
  <h4>"evil floating point bit level hacking"</h4>
  <p class="txt">
    Cette ligne semble assez compliquée mais elle est en vérité plutôt simple. 
    <br>
    On veut obtenir une variable de type "long", soit un nombre de même taille (en bits) qu'un flottant.
    <br>
    Problème : on ne veut pas que notre flottant soit tronqué. On va alors vouloir garder la même valeur binaire et simplement l'interpréter comme un long et pas comme un flottant.
    <br>
    Pour cela, on va prendre l'adresse de y, puis l'interpréter en adresse de long, puis récupérer la valeur à cette adresse.
    <br>
    <br>
    Pourquoi veut-on un long au lieu d'un flottant ? À cause du "bit-shifting"
    <br>
    Le bit-shifting consiste à prendre tous les bits d'un nombre et à les décaler vers la gauche ou vers la droite. 
    <br>
    Ainsi, Lorsqu'on bit-shift un nombre vers la gauche on le double et lorsqu'on le bit-shift vers la droite on le divise par 2 (en arrondissant à l'inférieur lorsque le résultat n'est pas un entier relatif)
    <br>
    On va donc pouvoir diviser par 2 notre flottant, et c'est ce que l'on veut. 
  </p>
  <h4>"What the f*ck"</h4>
  <p class="txt">
    on veut obtenir la racine carrée inverse, soit \(\frac{1}{\sqrt{x}}\)
    <br>
    on sait que \(x^{-1} = \frac{1}{x}\) et que \(x^\frac{1}{2} = \sqrt x\)
    <br>
    Ainsi, en combinant les 2, on obtient : \(x^{-\frac{1}{2}} = \frac{1}{\sqrt{x}}\)
    <br>
    Puisque nous avons démontré que \(x \approx log_2{x}\), on va donc pouvoir avoir :
    $$x \approx \log_2{x} \Rightarrow x^{-\frac{1}{2}} \approx \log_2(x^{-\frac{1}{2}})$$
    $$\log_2(x^{-\frac{1}{2}}) = -\frac{1}{2}\times\log_2{x}$$
    Vous allez alors peut-être vous dire qu'il y a une division dans cet algorithme, mais nous avons précédemment parlé de cette technique s'appliquant aux longs, permettant de diviser un nombre par 2 sans avoir à faire de division
    Donc : \(-\frac{1}{2}\log_2(x) = \)<code class="language-clike"> - (i >> 1)</code>
    Mais pourquoi a-t-on cette constante "0x5f3759df" semblant insignifiante ?
    <br>
    Admettons que \(\Gamma = x^{-\frac{1}{2}}\), alors \(\log_2(\Gamma) =-\frac{1}{2}\times\log_2(x)\\)
    posons \(B_n\) la représentation binaire de \(n\\).
    En remplaçant cette égalité par la représentation binaire de \(x\) et \(\Gamma\\), on obtient :
    $$\frac{1}{2^{23}}B_\Gamma + \mu - 127 = -\frac{1}{2}\Big(\frac{1}{2^{23}}B_x + \mu - 127\Big)$$
    $$\frac{B_\Gamma}{2^{23}} = -\frac{1}{2}\Big(\frac{B_x}{2^{23}} - (127 - \mu)\Big) + (127-\mu)$$
    $$B_\Gamma = 2^{23}(-\frac{1}{2}(\frac{B_x}{L} - (127 - \mu)) + (127 - \mu)) $$
    $$B_\Gamma = -\frac{1}{2}2^{23}(\frac{B_x}{2^{23}} - (127 - \mu)) + 2^{23}(127-\mu)$$
    $$B_\Gamma = -\frac{1}{2}B_x + \frac{1}{2}2^{23}(127-\mu) + 2^{23}(127 - \mu)$$
    $$B_\Gamma = \frac{3}{2}2^{23}(127-\mu) - \frac{1}{2}B_x$$
    Donc, pour trouver la valeur binaire de \(\Gamma\), on va devoir soustraire \\(\frac{1}{2}B_x\) à \(\frac{3}{2}2^{23}(127-\mu)\\).
    <br>
    On a déjà montré que \(\frac{1}{2}B_x\\) pouvait se calculer avec la ligne <code class="language-clike"> - (i >> 1)</code>
    Ainsi, Nous n'avons plus qu'à calculer \(\frac{3}{2}2^{23}(127-\mu)\\).
    $$\Big(\frac{3}{2}2^{23}(127-\mu)\Big)_{10} \approx (1597463007)_{10} = (\text{0x5f3759df})_{16}$$
    Donc, on se retrouve avec ce code :
    <code class="language-clike">0x5f3759df - (i >> 1)</code>
  </p>
  <h4>La méthode de Newton-Raphson</h4>
  <p class="txt">
    Comme vous avez pu le constater, le code des lignes précédentes est très imprécis et peut donner des résultats assez éloignés de la réalité.
    Heureusement, un certain Isaac Newton a découvert une méthode pour se rapprocher de la racine d'une fonction.
    Si nous trouvons une fonction qui admet pour racine le résultat que nous recherchons, cette méthode est parfaitement ce qu'il nous faut !
    <br>
    soit :
    $$
    \begin{align*}
    f:x\rightarrow \frac{1}{\sqrt{x}} \qquad \forall \: x \in \mathbb{R}^* \\
    y = f(x) \qquad \frac{1}{y^2} = x \Leftrightarrow \frac{1}{y^2} - x = 0    
    \end{align*}
    $$
    Nous avons donc trouvé une fonction \(g : y \rightarrow \frac{1}{y^2} - x\) (avec x la valeur initiale et y le résultat de \(\text{q_rsqrt(x)}\)) qui s'annule lorsque \(\text{q_rsqrt(x)} = \frac{1}{\sqrt x}\) (lorsque \(\frac{1}{\frac{1}{\text{q_rsqrt(x)}^2}} = x\))
    <br>
    La méthode de Newton est la suivante : 
    <br>
    Pour se rapprocher de la racine d'une fonction à partir d'une valeur initiale \(a_0\), on trace la tangente de la courbe de la fonction au point d'abscisse \(a_0\), la valeur \(a_1\) est l'abscisse de la racine de la fonction définissant notre tangente.
    <br>
    \(T_{a_0} : f'(a_0)(x-a_0) + f(a_0)\\)
    <br>
    Donc l'equation \((E)\) définissant l'abscisse du point pour lequel \(T_{a_0}\) s'annule est définie de la manière suivante : \((E) : f'(a_0)(x-a_0) + f(a_0) = 0\\)
    <br>
    $$
    \begin{align*}
    f'(a_0)(x-a_0) + f(a_0) = 0 \: \Leftrightarrow \:
    xf'(a_0) - a_0f'(a_0) + f(a_0) = 0 \: \Leftrightarrow \:
    xf'(a_0) = a_0f'(a_0) - f(a_0) \: \Leftrightarrow \:
    x = \frac{a_0f'(a_0) - f(a_0)}{f'(a_0)} \: \Leftrightarrow \:
    x = a_0 - \frac{f(a_0)}{f'(a_0)}
    \end{align*}
    $$
    On se retrouve donc avec l'equation \(x = a_0 - \frac{f(a_0)}{f'(a_0)}\\)
    <br>
    on peut l'interprêter sous la forme d'une suite, ce qui nous donne la suite \((u_n)\) définie par la formule \(u_{n+1} = u_n - \frac{f(u_n)}{f'(u_n)}\\)
    <br>
    On va donc pouvoir simplement remplacer \(f(a_0)\) et \(f'(a_0)\) par notre fonction \(g\) :
    $$
    \begin{align*}
    g'(y) = -\frac{2}{y^3} \\
    u_{n+1} = u_n - \frac{\frac{1}{u_n^2} - x}{-\frac{2}{u_n^3}} = u_n - \frac{u_n^3(\frac{1}{u_n^2}-x)}{-2} = u_n + \frac{u_n - u_n^3x}{2} = \frac{u_n(3 - u^2x)}{2} 
    \end{align*}
    $$
    On se retrouve alors avec l'écriture \(\frac{u_n(3 - u^2x)}{2}\) soit \(u_n(\frac{3}{2} - \frac{x}{2} \times u_n \times u_n)\), soit la même chose que la ligne <code class="language-clike">y  = y * ( threehalfs - ( x2 * y * y ) );</code>
    
  </p>
  <h3 id="interet">Cet algorithme est-il toujours intéressant ?</h3>
  <p>
    La réponse est : Non.
    <br>
    Pourquoi ?
    <br>
    Les ordinateurs de l'époque étaient très lents, surtout dans le calcul avec des nombres flottants. Aujourd'hui, nos ordinateurs possèdent des cartes graphiques et celles-ci ainsi que nos processeurs possèdent des unités spécialement conçues pour le calcul de nombres flottants et aujourd'hui calculer avec la bibliotheque standarde math.h est parfois plus rapide que notre algorithme 
    <br>
    De plus, les jeux actuels demandent aux calculs d'être très précis, si on veut que l'algorithme de la racine carrée inverse rapide soit assez précis pour les jeux d'aujourd'hui, on devrait utiliser au minimum 6 itérations de la méthode de Newton. Notre algorithme n'aurait alors plus grand intérêt.
  </p>
  <h2>Doit-on alors privilégier un mauvais code ?</h2>
  <h3 id="def-bon-mauvais">Qu'appelle-t-on du "bon"/"mauvais" code ?</h3>
  <p class="txt">
    Les développeurs et ingénieurs fonctionnent rarement seuls, travaillent en équipe sur le même code. Chacun doit alors pouvoir comprendre le code de l'autre afin d'avancer dans le projet.
    <br>
    On a vu que cet algorithme, pour quelqu'un qui n'a reçu aucune explication sur le fonctionnement de celui-ci, est incompréhensible.
    <br>
    Même si, dans ce cas précis, on peut faire abstraction de l'algorithme et l'utiliser sans faire attention à son implémentation, ce n'est pas toujours le cas.
    <br>
    Ce que je vais appeler "mauvais code" par la suite sera uniquement un fonctionnement obscure et peu compréhensible. (Un bon code est aussi caractérisé par le suivi de standards ainsi que la commentation du code mais ces critères n'affectent pas les performances) 
  </p>
  <h3 id="perf-ou-bon">Préférer les performances au bon code est-il toujours en vigueur aujourd'hui ? Si oui, dans quel cas ?</h3>
  <p class="txt">
    À l'époque de Quake, les performances comptaient énormément. On se devait alors évidemment de mettre en place les algorithmes les plus optimisés possible afin d'obtenir des performances décentes.
    Mais maintenant, avec toutes les technologies que l'on possède, les développeurs de jeux se posent de moins-en-moins la question de l'optimisation.
    Les gains de vitesse par de tels algorithmes sont de moins en moins visibles, on préfèrera se concentrer sur des problèmes de complexité plutôt que des problèmes de vitesse. 
    De plus, ces gains de vitesse se voient encore moins "à cause" de la vectorialisation.
    <br>
    Le problème est le suivant : plus les ordinateurs sont puissants, moins les développeurs optimisent.
    On en arrive à un point où certains jeux ne tournent pas bien sur des ordinateurs ou consoles récent(e)s.
    <br>
    Le jeu vidéo étant devenu industrie très importante, les développeurs se concentrent majoritairement sur le contenu plutôt que sur les performances.
    <br>
    Ont-ils raison ? Dans l'absolu, non puisqu'il est plus important de pouvoir jouer dans des conditions décentes. Mais les conditions de travail des studios de jeux vidéos sont tellement intenses qu'on comprend qu'ils ne prennent pas le temps d'optimiser
    <br>
    Il existe cependant des ingénieurs qui cherchent toujours à avoir les meilleurs performances, notamment les développeurs web, car ils utilisent le langage javascript, un langage interprêté et donc lent par définition.
    <br>
    Par exemple, l'entreprise Meta a récemment déployé le moteur Hermes, permettant un lancement plus rapide des application mobiles conçues avec react native, un outil de développement d'application mobiles en javascript.
  </p>
  <p>

  <hr>
  <h3 id="bibliographie">Bibliographie</h3>
  <a href="https://www.beyond3d.com/content/articles/15/">"Origin of Quake3's Fast InvSqrt()" - Beyond3D</a>
  <a href="https://en.wikipedia.org/wiki/IEEE_754-1985">IEEE 754-1985 - Wikipedia</a>
  <a href="https://fr.wikipedia.org/wiki/Racine_carr%C3%A9e_inverse_rapide">Racine carrée inversée - Wikipedia</a>
  <a href="https://www.geeksforgeeks.org/compiling-a-c-program-behind-the-scenes/">Compiling a C Program: Behind the Scenes (Geeks for Geeks)</a>
  <a href="https://www.youtube.com/watch?v=p8u_k2LIZyo">"Fast Inverse Square Root — A Quake III Algorithm" - Nemean (YouTube)</a>
  <a href="https://www.youtube.com/watch?v=NCuf2tjUsAY">The Fast Inverse Square Root -- 0x5f3759df explained!!</a>
  <a href="https://www.youtube.com/watch?v=tmb6bLbxd08">"The Truth about the Fast Inverse Square Root on the N64" - Kaze Emmanuar (YouTube)</a>
  <a href="https://www.youtube.com/watch?v=4LiP39gJuqE">"Optimizing with "Bad Code"" - Kaze Emmanuar (YouTube)</a>
  <a href="https://youtu.be/uCv5VRf8op0">"REVEALED: Quake III's SECRET Algorithm!" - Dave's Garage (YouTube)</a>
  <a href="https://www.linkedin.com/pulse/fast-inverse-square-root-still-armin-kassemi-langroodi">"Is Fast Inverse Square Root still Fast?" - Armin Kassemi Langroodi (LinkedIn)</a>
  <a href="http://www.matrix67.com/data/InvSqrt.pdf">"Fast Inverse Square Root" - Chris Lomont</a>

  <script src="load.js"></script>
  <script type="module" src="main.js"></script>
  <script type="module" src="animation.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
  <script>
  </script> 
</body>
</html>